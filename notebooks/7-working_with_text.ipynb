{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPMc1abLp0iBSXCm9x5ozUe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Working with Text\n","\n","So far we have only been working with images as inputs for our Neural Networks.\n","\n","When dealing with text inputs, such as documents, articles, or social media posts, the architecture of our current neural networks falls short because we need numerical input values as inputs for our networks.\n","\n","Text data is inherently different from images; it's unstructured, variable in length, and lacks the same spatial relationships that images possess. This necessitates a different approach to processing and understanding text data.\n","\n","One approach to solve this problem is, to split the text into words or subwords, called **tokens**, and convert these tokens into numerical representations that our neural networks can understand and analyze.\n","\n","This process is known as **text embedding** or vectorization of our tokens, and enables neural networks to comprehend and derive meaningful insights from textual information.\n","\n","But how do we get meaningful vector representations of our tokens? Well, thats the neat part; we treat them as learnable parameters and let the model come up with fitting word embeddings."],"metadata":{"id":"u2XU_NcM2MUZ"}},{"cell_type":"markdown","source":["## AG's News Corpus\n","\n","Let's see what a text classification network would look like using the example of the AG's News Corpus dataset.\n","\n","The AG's News Corpus is a dataset commonly used for text classification tasks. It consists of news articles categorized into four classes: *World*, *Sports*, *Business*, and *Science/Technology*. It contains $30,000$ training and $1,900$ test samples per class."],"metadata":{"id":"w887_e54Rswu"}},{"cell_type":"code","source":["!wget https://hyperion.bbirke.de/data/datasets/ag_news.zip\n","!mkdir -p datasets/ag_news\n","!unzip ag_news.zip -d datasets/ag_news/"],"metadata":{"id":"SGFPdz9rTbnu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Import of our Python packages."],"metadata":{"id":"aVreyUI454wI"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","\n","import numpy as np\n","import pandas as pd\n","import seaborn as sn\n","\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator"],"metadata":{"id":"3IVcaqpO4ve1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Training and test split are stored as `.csv` files. We load them as `pandas.DataFrame` for convenient data manipulation and visualization."],"metadata":{"id":"_2gnEyZl5--F"}},{"cell_type":"code","source":["df_train = pd.read_csv(\"datasets/ag_news/train.csv\")\n","df_test = pd.read_csv(\"datasets/ag_news/test.csv\")"],"metadata":{"id":"Q3RlpeSJUHKU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We print the first few rows and header of our DataFrame."],"metadata":{"id":"N7qNnUs47IfY"}},{"cell_type":"code","source":["df_train.head()"],"metadata":{"id":"L1hLhJkLURaR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Because the dataset starts with class 1 instead of 0, we decrease our labels by one."],"metadata":{"id":"ULbr7alE604O"}},{"cell_type":"code","source":["df_train[\"Class Index\"] = df_train[\"Class Index\"] - 1\n","df_test[\"Class Index\"] = df_test[\"Class Index\"] - 1"],"metadata":{"id":"xXvSjg92Uh-G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val2label = {\n","    0: \"World\",\n","    1: \"Sports\",\n","    2: \"Business\",\n","    3: \"Sci/Tech\"\n","}"],"metadata":{"id":"vn4l08FCVEAp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can create our `NewsDataset`."],"metadata":{"id":"cmRh6K0OAPkb"}},{"cell_type":"code","source":["class NewsDataset(Dataset):\n","    def __init__(self, texts, labels, transform=None, target_transform=None):\n","        self.transform = transform\n","        self.target_transform = target_transform\n","        self.labels = labels\n","        self.texts = texts\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = self.texts[idx]\n","        label = self.labels[idx]\n","        if self.transform:\n","            text = self.transform(text)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","        return text, label"],"metadata":{"id":"mJjhf6biWKnh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We create a training and testing instance of our datasets."],"metadata":{"id":"e2-DGiW8Aggr"}},{"cell_type":"code","source":["train_data = NewsDataset(\n","        texts=df_train[\"Description\"].to_list(),\n","        labels=df_train[\"Class Index\"].to_list()\n","    )\n","test_data = NewsDataset(\n","        texts=df_test[\"Description\"].to_list(),\n","        labels=df_test[\"Class Index\"].to_list()\n","    )"],"metadata":{"id":"-rudkB4rW5sV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's test our dataset and print some random texts and corresponding labels from our corpus."],"metadata":{"id":"36Btna8MArob"}},{"cell_type":"code","source":["examples = 10\n","\n","for i in range(examples):\n","    sample_idx = torch.randint(len(train_data), size=(1,)).item()\n","    text, label = train_data[sample_idx]\n","    print('News Text:')\n","    print(text)\n","    print(f'Category: {val2label[label]}')\n","    print('-'*30 + '\\n')"],"metadata":{"id":"VRCAGWEIXj1t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Until now, we've been treading familiar ground. However, it's time to dive into the realm of Natural Language Processing.\n","\n","Let's start with the tokenization."],"metadata":{"id":"ZbGtwyitA966"}},{"cell_type":"code","source":["tokenizer = get_tokenizer('basic_english')"],"metadata":{"id":"QhpkndcXVEFR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here, we created a simple tokenizer, which splits sentences into a list of tokens. A sentence is split on whitespaces and punctuation characters. Resulting tokens are then converted to lower case characters."],"metadata":{"id":"vR22t2UmHDAr"}},{"cell_type":"code","source":["tokenizer(\"This is a simple Test.\")"],"metadata":{"id":"LB3XL7T7VEH1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The next step is to construct a vocabulary using the raw training dataset. In this step, we utilize the PyTorch function `build_vocab_from_iterator`, which takes an iterator that yields a list or an iterator of tokens.\n","\n","We also add the custom token `<unk>` to our vocabulary, which we set as our default index. This token represents unknown words, which we have not encountered in our training dataset and is used as fallback index in case we would find an unseen token."],"metadata":{"id":"qQmFLhQIIH9g"}},{"cell_type":"code","source":["def yield_tokens(dataset):\n","    for idx in range(len(dataset)):\n","        text, _ = dataset[idx]\n","        yield tokenizer(text)\n","\n","vocab = build_vocab_from_iterator(yield_tokens(train_data), specials=[\"<unk>\"])\n","vocab.set_default_index(vocab[\"<unk>\"])"],"metadata":{"id":"hgRN0mqUVEKC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's see if everything works."],"metadata":{"id":"AiHrozKyKcCX"}},{"cell_type":"code","source":["tokens = tokenizer(\"This is a simple test.\")\n","indices = vocab(tokens)\n","reversed_tokens = vocab.lookup_tokens(indices)\n","\n","print(tokens)\n","print(indices)\n","print(reversed_tokens)"],"metadata":{"id":"JIp0lGAWdjby"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["the `collate_fn` parameter is used in conjunction with the `DataLoader` class, specifically when dealing with datasets that contain samples of varying sizes or shapes.\n","\n","When you have a dataset with samples that have different shapes or sizes, you often need to pad or resize them to make them uniform before feeding them into a neural network for training.\n","\n","The `collate_fn` parameter allows you to define a custom function that specifies how to collate (combine) the individual samples into batches.\n","\n","Given the variable lengths of our sentences, we consolidate our token indices and labels for the minibatch into a single dimension. Additionally, to maintain clarity regarding the starting point of each text sequence, we'll include offsets that are fed into our model alongside our regular inputs."],"metadata":{"id":"B4EJqt8NLaTP"}},{"cell_type":"code","source":["text_processor = lambda x: vocab(tokenizer(x))\n","label_processor = lambda x: int(x)\n","\n","def collate_batch(batch):\n","    label_list, text_list, offsets = [], [], [0]\n","    for _text, _label in batch:\n","        label_list.append(label_processor(_label))\n","        processed_text = torch.tensor(text_processor(_text), dtype=torch.int64)\n","        text_list.append(processed_text)\n","        offsets.append(processed_text.size(0))\n","    label_list = torch.tensor(label_list, dtype=torch.int64)\n","    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n","    text_list = torch.cat(text_list)\n","    return text_list, label_list, offsets"],"metadata":{"id":"in6URWzUY0RT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We define our hyperparameters."],"metadata":{"id":"ZMa_nHR4NTa9"}},{"cell_type":"code","source":["batch_size = 32\n","epochs = 10\n","learning_rate = 1e-3"],"metadata":{"id":"cgtoscSrmiZ8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can create an instance of our DataLoader with our custom collate function."],"metadata":{"id":"LnQTS-NoPUzm"}},{"cell_type":"code","source":["train_dataloader = DataLoader(\n","        train_data,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        collate_fn=collate_batch,\n","    )\n","\n","test_dataloader = DataLoader(\n","        test_data,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        collate_fn=collate_batch,\n","    )"],"metadata":{"id":"lxHzsIabVEOE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The model is rather minimal. We use an `nn.EmbeddingBag` layer, which computes the sum of our token embeddings in a sequence. The resulting tensor is then fed forward to a simple fully connected layer, as we have already seen.\n","\n","Note that the `forward` function now also requires offsets for the embedding layer."],"metadata":{"id":"-QEN8gCWPppz"}},{"cell_type":"code","source":["class EmbeddingModel(nn.Module):\n","    def __init__(self, vocab_size, embed_dim):\n","        super().__init__()\n","        self.embedding = nn.EmbeddingBag(\n","                num_embeddings=vocab_size,\n","                embedding_dim=embed_dim,\n","                sparse=False\n","            )\n","        self.linear_stack = nn.Sequential(\n","            nn.Linear(in_features=embed_dim, out_features=8),\n","            nn.ReLU(),\n","            nn.Dropout(p=0.2),\n","            nn.Linear(8, 4)\n","        )\n","\n","    def forward(self, text, offsets):\n","        embedded = self.embedding(text, offsets)\n","        return self.linear_stack(embedded)"],"metadata":{"id":"om58TnckbNDd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We define our device and create an instance of our model."],"metadata":{"id":"gvnnffrDRR_v"}},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\""],"metadata":{"id":"zOYiL9cnqgxr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = EmbeddingModel(vocab_size=len(vocab), embed_dim=16).to(device)"],"metadata":{"id":"MMGu3Es5bNGA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's check the forward pass."],"metadata":{"id":"4LzmCOB5RZ-W"}},{"cell_type":"code","source":["texts, labels, offsets = next(iter(train_dataloader))\n","\n","texts = texts.to(device)\n","offsets = offsets.to(device)\n","logits = model(texts, offsets)\n","pred_probab = nn.Softmax(dim=1)(logits)\n","y_pred = pred_probab.argmax(1)\n","\n","print(f\"Predicted class: {y_pred}\")"],"metadata":{"id":"T9CQ5c0bbNKV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We define our loss and optimizer."],"metadata":{"id":"qUPMxko7SQEY"}},{"cell_type":"code","source":["loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"],"metadata":{"id":"8OUoMutUs1ix"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The training and test loops are almost identical to what we have seen so far. The only difference is that our dataloader now also returns the offset, which is then fed into our model."],"metadata":{"id":"9L5bdjdHSTZu"}},{"cell_type":"code","source":["def train_loop(dataloader, model, loss_fn, optimizer):\n","    size = len(dataloader.dataset)\n","    model.train()\n","    for batch, (X, y, offset) in enumerate(dataloader):\n","        X = X.to(device)\n","        y = y.to(device)\n","        offset = offset.to(device)\n","        pred = model(X, offset)\n","        loss = loss_fn(pred, y)\n","\n","        # Backpropagation\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","        if batch % 100 == 0:\n","            loss, current = loss.item(), batch * batch_size + len(X)\n","            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n","\n","\n","def test_loop(dataloader, model, loss_fn, best_result):\n","    model.eval()\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    test_loss, correct = 0, 0\n","\n","    with torch.no_grad():\n","        for X, y, offset in dataloader:\n","            X = X.to(device)\n","            y = y.to(device)\n","            offset = offset.to(device)\n","            pred = model(X, offset)\n","            test_loss += loss_fn(pred, y).item()\n","            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","\n","    test_loss /= num_batches\n","    correct /= size\n","    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n","    if correct > best_result:\n","        print(\"New highscore! Saving model...\\n\")\n","        torch.save(model.state_dict(), 'best-model-parameters.pt')\n","        return correct\n","    print()\n","    return best_result"],"metadata":{"id":"WrNCGYINs4qN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's start the training loop!"],"metadata":{"id":"JG02vfsESyoq"}},{"cell_type":"code","source":["best_result = 0.0\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train_loop(train_dataloader, model, loss_fn, optimizer)\n","    best_result = test_loop(test_dataloader, model, loss_fn, best_result)\n","print(\"Done!\")"],"metadata":{"id":"tBgB9sj9tX3t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load the weights of our best model."],"metadata":{"id":"QxymIVr7S54p"}},{"cell_type":"code","source":["model.load_state_dict(torch.load(\"best-model-parameters.pt\"))"],"metadata":{"id":"LRkAcop51HGY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's print some example texts alongside predicted and actual labels."],"metadata":{"id":"vktkA-JFTB2v"}},{"cell_type":"code","source":["model.eval()\n","\n","examples = 10\n","\n","for i in range(examples):\n","    sample_idx = torch.randint(len(test_data), size=(1,)).item()\n","    text, actual_label = test_data[sample_idx]\n","    text_tensor, actual_label_tensor, offset_tensor = collate_batch([(text, actual_label)])\n","    text_tensor = text_tensor.to(device)\n","    offset_tensor = offset_tensor.to(device)\n","    logits = model(text_tensor, offset_tensor)\n","    pred_probab = nn.Softmax(dim=1)(logits)\n","    y_pred = pred_probab.argmax(1)\n","    y_prob = pred_probab.max()\n","    print('News Text:')\n","    print(text)\n","    print(f'Predicted Category: \\\"{val2label[y_pred.item()]}\\\" with p={y_prob.item():.3f}')\n","    print(f'Actual Category: \\\"{val2label[actual_label]}\\\"')\n","    print('-'*30 + '\\n')"],"metadata":{"id":"Lup0larO1KHX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can also check the confusion matrix."],"metadata":{"id":"DqgyWulaTMv8"}},{"cell_type":"code","source":["model.eval()\n","y_pred = []\n","y_true = []\n","\n","with torch.no_grad():\n","    # iterate over test data\n","    for X, y, offset in test_dataloader:\n","\n","            X = X.to(device)\n","            y = y.to(device)\n","            offset = offset.to(device)\n","            output = model(X, offset)\n","\n","            output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n","            y_pred.extend(output)\n","\n","            labels = y.data.cpu().numpy()\n","            y_true.extend(labels)\n","\n","cf_matrix = confusion_matrix(y_true, y_pred)\n","df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=0)[:, None], index = [val2label[i] for i in range(4)],\n","                     columns = [val2label[i] for i in range(4)])\n","plt.figure(figsize = (12,8))\n","sn.heatmap(df_cm, annot=True)"],"metadata":{"id":"GEBfq7KZ5Gh4"},"execution_count":null,"outputs":[]}]}